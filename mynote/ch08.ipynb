{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層を深くするということ\n",
    "--\n",
    "\n",
    "- 層を深くすることに比例して、認識性能も向上している（らしい）\n",
    "- 層を深くしたネットワークは、そうでない場合に比べて少ないパラメータでより良い表現力を達成できる\n",
    "- 小さなフィルタを重ねてネットワークを深くすることで、パラメータ数を減らし、受容野(receptive field)を広くカバーできる\n",
    "- 学習が効率的になる（層が深いと、各層で学習する問題を単純化できる）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "VGG\n",
    "\n",
    "- 畳み込み層とプーリング層から構成される基本的なCNN\n",
    "- 16-19層まで重ねる\n",
    "- シンプルな構成で応用性が高いため、広く使われている\n",
    "\n",
    "GoogLeNet\n",
    "\n",
    "- 基本的にはCNNと同じ\n",
    "- ネットワークが縦だけではなく、横の広がり、幅を持っている\n",
    "- サイズの異なるフィルタを複数適用して、結果を結合する\n",
    "- 1x1の畳み込み演算により、チャンネルを減らしている\n",
    "\n",
    "ResNet\n",
    "\n",
    "- 層を深くする際に、層をまたぐようにするスキップ構造を採用\n",
    "- 層を深くしても効率よく学習できるようになった\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高速化\n",
    "--\n",
    "\n",
    "GPU\n",
    "\n",
    "- 行列などの並列的な演算はGPUが得意\n",
    "- NVIDIAのCUDAがよく使われている\n",
    "\n",
    "分散\n",
    "\n",
    "- 複数GPUや複数マシンによる分散学習\n",
    "- TensorFlow、CNTKなど\n",
    "\n",
    "演算精度\n",
    "\n",
    "- 数値精度はそこまで必要とされない（ノイズがのっても影響がほぼない）\n",
    "- 16bitの半精度浮動小数点数(half float)でも良い\n",
    "- ビット数の削減に関しては、組み込み向けにおいて今後重要なテーマ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
